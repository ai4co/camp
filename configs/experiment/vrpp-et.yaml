# @package _global_

# Override defaults: take configs from relative path
defaults:
  - override /model: et.yaml
  - override /env: vrpp-seq.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml
  - override /logger: wandb.yaml

env:
  # ET has different step function
  _target_: "camp.baselines.et.envs.vrpp_seq.env.VRPPreferenceSeqEnv"

generator_params:
  num_loc: 100 # max locs
  num_agents: 7 # max agents

# Model hyperparameters
model:
  policy:
    _target_: "camp.baselines.et.policy.EquityTransformerPolicy"
    env_name: "vrpp_seq"
    encoder:
      _target_: "camp.baselines.et.encoder.EquityTransformerEncoder"
      num_heads: 8
      embed_dim: 128
      num_layers: 3
      env_name: "vrpp_seq"
      normalization: "rezero"
    decoder:
      _target_: "camp.baselines.et.decoder.EquityTransformerDecoder"
      embed_dim: 128
      num_heads: 8
      env_name: "vrpp_seq"
      use_graph_context: True
    multi_objective_weights: [0.9, 0.1]
  batch_size: 64
  val_batch_size: ${model.batch_size}
  test_batch_size: ${model.batch_size}
  train_data_size: 100_000
  val_data_size: 10_000
  test_data_size: 10_000
  optimizer_kwargs:
    lr: 1e-4
    weight_decay: 0.0
  lr_scheduler:
    "MultiStepLR"
  lr_scheduler_kwargs:
    milestones: [80, 95]
    gamma: 0.1
  num_augment: 8
  train_min_agents: 3
  train_max_agents: ${env.generator_params.num_agents} # max agents is the number of agents
  train_min_size: 40
  train_max_size: ${env.generator_params.num_loc} # max size is the number of locations
  metrics:
    train: ["loss", "reward", "preference", "time"]
    val: ["reward", "preference", "time"]

# Logging: we use Wandb in this case
logger:
  wandb:
    project: "vrp-preference"
    tags: ["et"]
    group: "n${env.generator_params.num_loc}_m${env.generator_params.num_agents}_w${model.policy.multi_objective_weights}"
    name: "et_w${model.policy.multi_objective_weights}"

# Trainer: this is a customized version of the PyTorch Lightning trainer.
trainer:
  max_epochs: 100

seed: 1234

# Used to save the best model. However, we are not using this in the current setup
callbacks:
  model_checkpoint:
    monitor: "val/reward/n40_m3_random"
