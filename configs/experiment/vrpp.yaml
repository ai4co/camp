# @package _global_

# Override defaults: take configs from relative path
defaults:
  - override /model: parco.yaml
  - override /env: vrpp.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml
  - override /logger: wandb.yaml

generator_params:
  num_loc: 100 # max locs
  num_agents: 7 # min locs

# Model hyperparameters
model:
  policy:
    _target_: "camp.models.policy.PARCOPolicy"
    env_name: "vrpp"
    encoder:
      _target_: "camp.models.encoder.VRPPEncoder"
      num_heads: 8
      embed_dim: 128
      num_layers: 3
      normalization: "instance"
      use_final_norm: False
      norm_after: False
    decoder:
      _target_: "camp.models.decoder.VRPPDecoder"
      num_heads: 8
      embed_dim: 128
      use_graph_context: False
    agent_handler: "highprob"
    embed_dim: 128
    norm_after: false # true means we use Kool structure, which seems slightly worse
    normalization: "rms"
    use_final_norm: true # LLM way
    parallel_gated_kwargs:
      mlp_activation: "silu" # use MLP as in LLaMa
    context_embedding_kwargs:
      normalization: "rms"
      norm_after: false # true means we use Kool structure, which seems slightly worse
      use_final_norm: true # LLM way
      parallel_gated_kwargs:
        mlp_activation: "silu"
    multi_objective: True
    multi_objective_weights: [0.8, 0.2]
  batch_size: 128
  val_batch_size: ${model.batch_size}
  test_batch_size: ${model.batch_size}
  train_data_size: 100_000
  val_data_size: 10_000
  test_data_size: 10_000
  optimizer_kwargs:
    lr: 1e-4
    weight_decay: 0.0
  lr_scheduler:
    "MultiStepLR"
  lr_scheduler_kwargs:
    milestones: [80, 95]
    gamma: 0.1
  num_augment: 8
  train_min_agents: 3
  train_max_agents: ${env.generator_params.num_agents} # max agents is the number of agents
  train_min_size: 40
  train_max_size: ${env.generator_params.num_loc} # max size is the number of locations
  metrics:
    train: ["loss", "reward", "preference", "time", "halting_ratio", "pos_ratio"]
    val: ["reward", "preference", "time", "halting_ratio", "pos_ratio"]

# Logging: we use Wandb in this case
logger:
  wandb:
    project: "vrp-preference"
    tags: ["camp"]
    group: "n${env.generator_params.num_loc}_m${env.generator_params.num_agents}_w${model.policy.multi_objective_weights}"
    name: "camp_w${model.policy.multi_objective_weights}"

# Trainer: this is a customized version of the PyTorch Lightning trainer.
trainer:
  max_epochs: 100

seed: 1234

# Used to save the best model. However, we are not using this in the current setup
callbacks:
  model_checkpoint:
    monitor: "val/reward/n40_m3_random"
